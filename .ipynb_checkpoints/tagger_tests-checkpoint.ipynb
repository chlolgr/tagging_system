{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk import word_tokenize,pos_tag\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pickle as pkl\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables():\n",
    "    idfs = pkl.load(open('idfs_dict_lemm.pkl','rb'))\n",
    "    vocab = list(idfs.keys())\n",
    "    len_vocab = len(vocab)\n",
    "    word_to_int = dict(zip(vocab,range(len_vocab)))\n",
    "    idfs_array = np.zeros(len_vocab)\n",
    "    for word in vocab: idfs_array[word_to_int[word]] = idfs[word]\n",
    "    stopw = stopwords.words('english')\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    return vocab,len_vocab,word_to_int,idfs_array,stopw,lemmer,nlp\n",
    "\n",
    "def pt_to_wn(tag):\n",
    "    if tag.startswith('J'): return wordnet.ADJ\n",
    "    if tag.startswith('V'): return wordnet.VERB\n",
    "    if tag.startswith('N'): return wordnet.NOUN\n",
    "    if tag.startswith('R'): return wordnet.ADV\n",
    "\n",
    "def remove_unwanted(tags):\n",
    "    remove = list()\n",
    "    days = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "    day_of_the_month_re = '[1-9][1-9]?th'\n",
    "    time_re = '[0-1]?[0-9]:[0-5][0-9]([a,p]m)?'\n",
    "    belonging_re = '\\.\\'s'\n",
    "    for i,tag in enumerate(tags): \n",
    "        match = re.search(belonging_re,tag)\n",
    "        if match:\n",
    "            start,end = match.span()\n",
    "            tags[i] = tag[:start]+tag[end:]\n",
    "        for bit in tag.split('.'):\n",
    "            match = re.search(day_of_the_month_re,bit)\n",
    "            if match: \n",
    "                remove.append(tag)\n",
    "                break\n",
    "            match = re.search(time_re,bit)\n",
    "            if match: remove.append(tag)\n",
    "    for tag in remove: tags.remove(tag)\n",
    "    return [tag for tag in tags if tag not in days and len(tag)>1]\n",
    "\n",
    "def get_clean_text_raw(raw_text,stopw,lemmer):\n",
    "    text = word_tokenize(raw_text)\n",
    "    postags = pos_tag(text)\n",
    "    postags = [(word,tag) for word,tag in postags if tag in ['NN','JJ','JJR','NNS','VB','VBD','VBG','VBN','VBP','VBZ']]\n",
    "    text = [lemmer.lemmatize(word.lower(),pt_to_wn(tag)) for word,tag in postags if word.lower() not in stopw and word.isalnum() and not word.isdigit()]\n",
    "    postags = [tag for word,tag in postags if word.lower() not in stopw and word.isalnum() and not word.isdigit()]\n",
    "    return text,postags\n",
    "\n",
    "def get_TFs_raw(raw_text,len_vocab,word_to_int,stopw,lemmer):\n",
    "    words,postags = get_clean_text_raw(raw_text,stopw,lemmer)\n",
    "    word_counts = dict(zip(*np.unique(words,return_counts=True)))\n",
    "    n_words = sum(list(word_counts.values()))\n",
    "    tfs = np.zeros(len_vocab)\n",
    "    unknowns = list()\n",
    "    for i,word in enumerate(words): \n",
    "        try: tfs[word_to_int[word]] = word_counts[word]/n_words\n",
    "        except KeyError: unknowns.append((word,postags[i]))\n",
    "    return tfs,unknowns\n",
    "\n",
    "def get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,lemmer):\n",
    "    tfs,unknowns = get_TFs_raw(raw_text,len_vocab,word_to_int,stopw,lemmer)\n",
    "    tfidfs = tfs*idfs_array\n",
    "    order = np.flip(np.argsort(tfidfs))\n",
    "    significants = np.array(vocab)[order]\n",
    "    return list(significants),list(tfidfs[order]),unknowns\n",
    "\n",
    "def get_sentences(raw_text,punctuation=False):\n",
    "    raw_text = raw_text.replace('-','')\n",
    "    sentences = list()\n",
    "    sentence = ''\n",
    "    for character in raw_text:\n",
    "        if character in ['.','!','?']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = ''\n",
    "        else: sentence+=character\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        if punctuation: sentences[i] = [word for word in word_tokenize(sentence)]\n",
    "        else: sentences[i] = [word for word in word_tokenize(sentence) if word.isalnum() and not word.isdigit()]\n",
    "    return sentences\n",
    "\n",
    "def common_words(entity1,entity2):\n",
    "    words1 = entity1.split()\n",
    "    words2 = entity2.split()\n",
    "    for word in words1:\n",
    "        if word in words2: return True\n",
    "    return False\n",
    "\n",
    "def process_ppl_org(entities,len_text):\n",
    "    counts = dict()\n",
    "    cleaned_entities = list()\n",
    "    for entity in entities: \n",
    "        add = True\n",
    "        if entity.label_=='ORG':\n",
    "            cleaned_entities.append(entity.text)\n",
    "            continue\n",
    "        if entity.label_=='PERSON': \n",
    "            for r in counts: \n",
    "                if common_words(r,entity.text): \n",
    "                    counts[r]+=1\n",
    "                    add=False\n",
    "                    if len(entity.text.split())>len(r.split()): \n",
    "                        counts[entity.text] = counts[r]\n",
    "                        del counts[r]\n",
    "                    break\n",
    "            if add: counts[entity.text] = 1\n",
    "    if len(counts)>0:\n",
    "        average_appearance = np.mean(list(counts.values()))\n",
    "        all_words = list()\n",
    "        for entity,count in counts.items():\n",
    "            if count>=average_appearance: \n",
    "                cleaned_entities.append(entity)\n",
    "    for i,entity in enumerate(cleaned_entities): \n",
    "        cleaned_entities[i] = ' '.join([word for word in entity.split() if word.isalnum()])\n",
    "    return cleaned_entities\n",
    "\n",
    "def process_places(places,len_text,stopw,lemmer):\n",
    "    keeps = list()\n",
    "    average_count = np.mean(list(places.values()))\n",
    "    for place,counts in places.items(): \n",
    "        if counts>=average_count: \n",
    "            keeps.append('.'.join([word.lower() for word in place.split() if word.lower() not in stopw]))\n",
    "    return keeps\n",
    "\n",
    "def get_spacy_entities(raw_text,stopw,nlp,lemmer):\n",
    "    sentences = get_sentences(raw_text,punctuation=True)\n",
    "    len_text = sum([len(sentence) for sentence in sentences])\n",
    "    entities = list()\n",
    "    places = dict()\n",
    "    for sentence in sentences: \n",
    "        sentence = ' '.join(sentence)\n",
    "        doc = nlp(sentence)\n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ in ['PERSON','ORG'] and len(entity.text.split())<=3: entities.append(entity)\n",
    "            if entity.label_=='GPE': \n",
    "                try: places[entity.text] += 1\n",
    "                except KeyError: places[entity.text] = 1\n",
    "    entities = process_ppl_org(entities,len_text)\n",
    "    for i,entity in enumerate(entities):\n",
    "        entities[i] = '.'.join([word.lower() for word in entity.split() if word.lower() not in stopw])\n",
    "    places = process_places(places,len_text,stopw,lemmer)\n",
    "    return entities+places\n",
    "\n",
    "def process_unknowns(unknowns):\n",
    "    porter = PorterStemmer()\n",
    "    keeps = list()\n",
    "    stems = dict()\n",
    "    for word,tag in unknowns:\n",
    "        stem = porter.stem(word)\n",
    "        try: stems[stem].append((word,tag))\n",
    "        except KeyError: stems[stem] = [(word,tag)]\n",
    "    for stem,appearances in stems.items():\n",
    "        if len(appearances)>1:\n",
    "            done = False\n",
    "            for word,tag in appearances:\n",
    "                if tag=='NN': \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances:\n",
    "                if tag.startswith('N'): \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances:\n",
    "                if tag=='JJ': \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances:\n",
    "                if tag.startswith('J'): \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances: \n",
    "                if tag.startswith('V'): \n",
    "                    keeps.append(word)\n",
    "                    break\n",
    "    return keeps\n",
    "\n",
    "def get_spacy_significants(raw_text,lemmer,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp):\n",
    "    entities = get_spacy_entities(raw_text,stopw,nlp,lemmer)\n",
    "    doubles = list()\n",
    "    for entity in entities: doubles += entity.split('.')\n",
    "    doubles = [tag for tag in doubles]\n",
    "    significants,tfidfs,unknowns = get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,lemmer)\n",
    "    tags = list()\n",
    "    for i,s in enumerate(significants): \n",
    "        if tfidfs[i]<.0005: break\n",
    "        if s not in doubles: tags.append(s)\n",
    "    tags += process_unknowns(unknowns)\n",
    "    tags += entities\n",
    "    tags = remove_unwanted(tags)\n",
    "    return list(set(tags))\n",
    "\n",
    "def get_tags(raw_text):\n",
    "    vocab,len_vocab,word_to_int,idfs_array,stopw,lemmer,nlp = initialize_variables()\n",
    "    return get_spacy_significants(raw_text,lemmer,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#facebook\n",
      "#google\n",
      "#france\n",
      "#bruno.le.maire\n",
      "#optimize\n",
      "#amazon\n",
      "#french\n",
      "#le.maire\n",
      "#unanimous\n",
      "#marketplace\n",
      "#taxation\n",
      "#american\n",
      "#weird\n",
      "#mailing\n",
      "#european\n",
      "#uber\n",
      "#tech\n",
      "\n",
      "time: 0.8228862285614014\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Is it happening? Is it not happening? After years of back and forth, it looks like the new tax on tech giants in France is about to become a law. Big tech companies that generate significant revenue in France will be taxed on their revenue generated in France.\n",
    "\n",
    "France’s Economy Minister Bruno Le Maire has been lobbying for a new tax so that tech giants would stop optimizing their European corporate structure to lower their effective tax rate. Originally, Le Maire wanted to convince other European countries to get on board.\n",
    "\n",
    "But you need a unanimous vote when it comes to tax reforms in Europe. And Le Maire couldn’t convince everyone.\n",
    "\n",
    "Le Maire still wanted to do something. So here we are, with a new tax on tech companies that generate over €750 million in revenue globally and €25 million in France.\n",
    "\n",
    "If you’re operating a marketplace (Amazon’s marketplace, Uber, Airbnb…) or an advertising business (Facebook, Google, Criteo…), you will have to pay 3 percent of your French revenue in taxes. The government says that it isn’t against American companies as European and Asian companies are also about to get taxed.\n",
    "\n",
    "It’s a weird taxation model as it is based on revenue and not profit. It’ll also require some work from the taxation administration as French revenue means that it involves all transactions with somebody with a French mailing address or a French IP address. France expects to generate €400 million in revenue with this new tax in 2019.\n",
    "\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "tags = get_tags(text)\n",
    "stop = time.time()\n",
    "print('#'+'\\n#'.join(tags))\n",
    "print('\\ntime:',stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
