{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import treetaggerwrapper\n",
    "tagdir = '/home/chloe/Documents/snipfeed/prod/tagging_system/french/treetagger'\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pickle as pkl\n",
    "import unidecode\n",
    "import spacy\n",
    "\n",
    "\n",
    "def initialize_variables():\n",
    "    idfs = pkl.load(open('idfs_dict_lemm.pkl','rb'))\n",
    "    vocab = list(idfs.keys())\n",
    "    len_vocab = len(vocab)\n",
    "    word_to_int = dict(zip(vocab,range(len_vocab)))\n",
    "    idfs_array = np.zeros(len_vocab)\n",
    "    for word in vocab: idfs_array[word_to_int[word]] = idfs[word]\n",
    "    stopw = stopwords.words('french')\n",
    "    start = time.time()\n",
    "    return vocab,len_vocab,word_to_int,idfs_array,stopw\n",
    "\n",
    "def remove_unwanted(tags):\n",
    "    remove = list()\n",
    "    hour_re = '[0-2]?[0-9][h,:]([0-9]{2})?'\n",
    "    days = ['lundi','mardi','mercredi','jeudi','vendredi','samedi','dimanche']\n",
    "    for i,tag in enumerate(tags): \n",
    "        match = re.search(hour_re,tag)\n",
    "        if match: \n",
    "            remove.append(tag)\n",
    "            continue\n",
    "        tags[i] = tag.replace('_','')\n",
    "        #tags[i] = unidecode.unidecode(tag)\n",
    "        if '|' in tag: remove.append(tag)\n",
    "    for tag in remove: tags.remove(tag)\n",
    "    return [tag for tag in tags if tag not in days and len(tag)>1]\n",
    "\n",
    "def get_clean_text_raw(raw_text,stopw,complete=False):\n",
    "    raw_text = raw_text.replace('-',' ')\n",
    "    raw_text = raw_text.replace('’','\\'')\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr',TAGDIR=tagdir)\n",
    "    tags = tagger.tag_text(raw_text)\n",
    "    tags2 = treetaggerwrapper.make_tags(tags)\n",
    "    if complete: postags = [(tag.word.lower(),tag.pos) for tag in tags2 if isinstance(tag,treetaggerwrapper.Tag)]\n",
    "    else: postags = [(tag.lemma.lower(),tag.pos) for tag in tags2 if isinstance(tag,treetaggerwrapper.Tag) and tag.pos[:3] in ['NOM','NAM','VER','ABR','ADJ'] and tag.word.lower() not in stopw]\n",
    "    words = [word for word,tag in postags]\n",
    "    postags = [tag for word,tag in postags]\n",
    "    return words,postags\n",
    "\n",
    "def get_TFs_raw(raw_text,len_vocab,word_to_int,stopw):\n",
    "    words,postags = get_clean_text_raw(raw_text,stopw)\n",
    "    NEs = np.array(words)[np.where(np.array(postags).astype(str)=='NAM')[0]]\n",
    "    NE,counts = np.unique(NEs,return_counts=True)\n",
    "    average = np.mean(counts)\n",
    "    NEs = NE[np.where(counts>=average)[0]]\n",
    "    word_counts = dict(zip(*np.unique(words,return_counts=True)))\n",
    "    n_words = sum(list(word_counts.values()))\n",
    "    tfs = np.zeros(len_vocab)\n",
    "    unknowns = list()\n",
    "    for i,word in enumerate(words): \n",
    "        try: tfs[word_to_int[word]] = word_counts[word]/n_words\n",
    "        except KeyError: unknowns.append((word,postags[i]))\n",
    "    return tfs,unknowns,NEs\n",
    "\n",
    "def get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw):\n",
    "    tfs,unknowns,NEs = get_TFs_raw(raw_text,len_vocab,word_to_int,stopw)\n",
    "    tfidfs = tfs*idfs_array\n",
    "    order = np.flip(np.argsort(tfidfs))\n",
    "    significants = np.array(vocab)[order]\n",
    "    return list(significants),list(tfidfs[order]),unknowns,list(NEs)\n",
    "\n",
    "def get_sentences(raw_text,punctuation=False):\n",
    "    raw_text = raw_text.replace('-',' ')\n",
    "    raw_text = raw_text.replace('’','\\'')\n",
    "    sentences = list()\n",
    "    sentence = ''\n",
    "    for character in raw_text:\n",
    "        if character in ['.','!','?']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = ''\n",
    "        else: sentence+=character\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        if punctuation: sentences[i] = [word for word in word_tokenize(sentence,language='french')]\n",
    "        else: sentences[i] = [word for word in word_tokenize(sentence,language='french') if word.isalnum() and not word.isdigit()]\n",
    "    return sentences\n",
    "\n",
    "def process_unknowns(unknowns):\n",
    "    keeps = [word for word,tag in unknowns if tag[:3] in ['NOM','VER','NAM','ADJ'] and len(word)>=3]\n",
    "    keeps = dict(zip(*np.unique(keeps,return_counts=True)))\n",
    "    average = np.mean(list(keeps.values()))\n",
    "    keeps = [keep for keep,count in keeps.items() if count>=average]\n",
    "    return keeps\n",
    "\n",
    "def get_spacy_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw):\n",
    "    significants,tfidfs,unknowns,NEs = get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw)\n",
    "    tags = list()\n",
    "    for i,s in enumerate(significants): \n",
    "        if tfidfs[i]<3e-5: break\n",
    "        tags.append(s)\n",
    "    tags += process_unknowns(unknowns)\n",
    "    tags += NEs\n",
    "    tags = remove_unwanted(tags)\n",
    "    return list(set(tags))\n",
    "\n",
    "def parse_tuples(raw_text,stopw):\n",
    "    words,postags = get_clean_text_raw(raw_text,stopw,complete=True)\n",
    "    tuples = list()\n",
    "    for i in range(len(words)-1): \n",
    "        if ' '.join(postags[i:i+2]) not in ['NAM NAM','NOM ADJ','ADJ NOM','NOM NOM']: continue\n",
    "        tuples.append('.'.join(words[i:i+2]))\n",
    "    tuples,counts = np.unique(tuples,return_counts=True)\n",
    "    average = np.mean(counts)\n",
    "    return list(tuples[np.where(counts>=average)[0]])\n",
    "\n",
    "def get_tags(raw_text):\n",
    "    vocab,len_vocab,word_to_int,idfs_array,stopw = initialize_variables()\n",
    "    tuples_ = parse_tuples(raw_text,stopw)\n",
    "    doubles = list()\n",
    "    for tuple_ in tuples_: doubles+=tuple_.split('.')\n",
    "    tags = get_spacy_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw)\n",
    "    remove = list()\n",
    "    for tag in tags: \n",
    "        if tag in doubles: \n",
    "            remove.append(tag)\n",
    "            continue\n",
    "    for tag in remove: tags.remove(tag)\n",
    "    return tags+tuples_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librairie\n",
      "dataiku\n",
      "package\n",
      "keras\n",
      "cloud\n",
      "infrastructure\n",
      "microsoft\n",
      "frameworks\n",
      "tensorflow\n",
      "validation\n",
      "scikit\n",
      "calcul\n",
      "inria\n",
      "learn\n",
      "didactique\n",
      "ait.amir\n",
      "bouzid.ait\n",
      "deep.learning\n",
      "documentation.claire\n",
      "machine.learning\n",
      "open.source\n",
      "packages.prêts\n",
      "points.forts\n",
      "principaux.points\n",
      "python.software\n",
      "software.foundation\n",
      "\n",
      "time: 0.47754573822021484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"\"\"\n",
    "Le projet orchestré par l'INRIA devient l'infrastructure de machine learning de référence aux côtés des frameworks de deep learning tels que Keras ou Tensorflow.\n",
    "\n",
    "Initié et piloté en France par l'INRIA, le projet open source Scikit-learn est devenu une référence dans le monde de l'intelligence artificielle. De Paris à San Francisco en passant par Singapour, la bibliothèque de machine learning, écrite en Python, s'impose aux start-up jusqu'aux grands groupes, Gafam compris.\n",
    "\n",
    "Preuve de cet engouement, dans le dernier baromètre de la Python Software Foundation, Scikit-learn se hisse en cinquième position des frameworks et librairies de data science Python les plus utilisés tous domaines confondus. Il est cité par près d'un tiers des 20 000 développeurs interrogés. Et dans la catégorie des technologies d'apprentissage machine stricto sensu, elle occupe la première place du palmarès. Elle se classe même largement devant les infrastructures de deep learning Tensorflow et Keras qui, de leur côté, totalisent respectivement 25% et 15% des suffrages.\n",
    "\n",
    "Comment expliquer ce succès fulgurant ? Scikit-learn recouvre les principaux algorithmes de machine learning généralistes : classification, régression, clustering, gradient boosting... En parallèle, le framework embarque NumPy, Matplotlib et SciPy, trois librairies star du calcul scientifique comme l'illustre encore l'indice de la Python Software Foundation (dans lequel elles trustent successivement la première, troisième et quatrième position). \"Du coup, la communauté des chercheurs rompus au calcul matricielle l'a rapidement adoptée\", constate Aymen Chakhari, directeur IA et data science au sein de l'ESN Devoteam.\n",
    "\n",
    "Dataiku, Microsoft et IBM dans la boucle\n",
    "Par voie de conséquence, Scikit-learn est également la bibliothèque de machine learning la plus souvent présente dans les cursus d'enseignement supérieur en data science. Résultat : un large contingent de jeunes diplômés et d'experts prêts à la déployer en entreprise sont venus alimenter le marché de l'emploi, faisant encore monter la mayonnaise.\n",
    "\n",
    "\"L'un de ses principaux points forts : une documentation claire et didactique avec des packages prêts à l'emploi\"\n",
    "Quant aux fournisseurs d'outils d'IA commerciaux, ils ont rapidement vu dans Scikit-learn une potentielle poule aux œufs d'or. Sa disponibilité en open source (sous licence BSD) et son mode de développement communautaire ont fait le reste. L'infrastructure est désormais implémentée par plusieurs poids lourds de la data science dont le Français Dataiku, l'Américain DataRobot et l'Allemand Knime. Elle est aussi prise en charge par un nombre croissant d'acteurs du cloud. C'est le cas de Google via son service Cloud Machine Learning Engine, d'IBM avec Watson Machine Learning ou encore de Microsoft par le biais d'Azure Machine Learning.\n",
    "\n",
    "\"L'un des principaux points forts de Scikit-learn est de proposer une documentation claire et didactique avec des exemples d'implémentations et des packages prêts à l'emploi. Tensorflow se révèle beaucoup plus difficile à paramétrer\", compare Bouzid Ait Amir, responsable du pôle analytics chez Keyrus.\n",
    "\n",
    "De l'hyperparameter tuning\n",
    "Parmi ses facteurs de différentiation, Scikit-learn est plébiscitée pour sa méthode de validation croisée. \"En amont, elle fournit la possibilité de générer très simplement les bases d'entrainement et de test\", explique Bouzid Ait Amir. Ensuite via un mécanisme de grid search, la validation croisée permet de dénicher les paramètres du modèle se rapprochant le plus des prédictions attendues. Le processus ajuste l'échantillonnage de la base de test en la confrontant à la base d'apprentissage par itérations successives (voir schéma ci-dessous). \"L'objectif est d'aboutir au bon réglage en termes de seuils, par exemple ne pas dépasser 2% en matière de détection de fraudes\", détaille Bouzid Ait Amir.\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "tags = get_tags(text)\n",
    "print('\\n'.join(tags))\n",
    "print('\\ntime:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
