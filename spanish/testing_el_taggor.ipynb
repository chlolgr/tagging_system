{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk import word_tokenize,pos_tag\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pickle as pkl\n",
    "import spacy\n",
    "import treetaggerwrapper\n",
    "tagdir = '/home/chloe/Documents/snipfeed/prod/tagging_system/french/treetagger'\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables():\n",
    "    idfs = pkl.load(open('idfs_dict_lemm.pkl','rb'))\n",
    "    vocab = list(idfs.keys())\n",
    "    len_vocab = len(vocab)\n",
    "    word_to_int = dict(zip(vocab,range(len_vocab)))\n",
    "    idfs_array = np.zeros(len_vocab)\n",
    "    for word in vocab: idfs_array[word_to_int[word]] = idfs[word]\n",
    "    stopw = stopwords.words('spanish')\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    return vocab,len_vocab,word_to_int,idfs_array,stopw,nlp\n",
    "\n",
    "def remove_unwanted(tags):\n",
    "    remove = list()\n",
    "    days = ['lunes','martes','miércoles','jueves','viernes','sábado','domingo']\n",
    "    return [tag for tag in tags if tag not in days and len(tag)>1]\n",
    "\n",
    "def get_clean_text_raw0(raw_text,stopw,nlp):\n",
    "    sentences = get_sentences(raw_text,punctuation=True)\n",
    "    words = list()\n",
    "    postags = list()\n",
    "    for sentence in sentences: \n",
    "        doc = nlp(' '.join(sentence))\n",
    "        words += [token.lemma_.lower() for token in doc if token.text.lower() not in stopw and token.pos_ in ['PROPN','ADV','ADJ','NOUN','ADP','AUX']]\n",
    "        postags += [token.pos_ for token in doc if token.text.lower() not in stopw and token.pos_ in ['PROPN','ADV','ADJ','NOUN','ADP','AUX']]\n",
    "    return words,postags\n",
    "\n",
    "def get_clean_text_raw(raw_text,stopw):\n",
    "    words,postags = list(),list()\n",
    "    sentences = get_sentences(raw_text,punctuation=True)\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='es',TAGDIR=tagdir)\n",
    "    tags = tagger.tag_text(text)\n",
    "    tags2 = treetaggerwrapper.make_tags(tags)\n",
    "    for tag in tags2:\n",
    "        if tag.lemma.lower() not in stopw and ((tag.pos.startswith('N') and tag.pos!='NP') or tag.pos.startswith('V') or tag.pos in ['ADJ','ADV']): \n",
    "            words.append(tag.lemma.lower())\n",
    "            postags.append(tag.pos)\n",
    "    return words,postags\n",
    "\n",
    "def get_TFs_raw(raw_text,len_vocab,word_to_int,stopw,nlp):\n",
    "    words,postags = get_clean_text_raw(raw_text,stopw)\n",
    "    word_counts = dict(zip(*np.unique(words,return_counts=True)))\n",
    "    n_words = sum(list(word_counts.values()))\n",
    "    tfs = np.zeros(len_vocab)\n",
    "    unknowns = list()\n",
    "    for i,word in enumerate(words): \n",
    "        try: tfs[word_to_int[word]] = word_counts[word]/n_words\n",
    "        except KeyError: unknowns.append((word,postags[i]))\n",
    "    return tfs,unknowns\n",
    "\n",
    "def get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp):\n",
    "    tfs,unknowns = get_TFs_raw(raw_text,len_vocab,word_to_int,stopw,nlp)\n",
    "    tfidfs = tfs*idfs_array\n",
    "    order = np.flip(np.argsort(tfidfs))\n",
    "    significants = np.array(vocab)[order]\n",
    "    return list(significants),list(tfidfs[order]),unknowns\n",
    "\n",
    "def get_sentences(raw_text,punctuation=False):\n",
    "    raw_text = raw_text.replace('-','')\n",
    "    sentences = list()\n",
    "    sentence = ''\n",
    "    for character in raw_text:\n",
    "        if character in ['.','!','?']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = ''\n",
    "        else: sentence+=character\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        if punctuation: sentences[i] = [word for word in word_tokenize(sentence,language='spanish')]\n",
    "        else: sentences[i] = [word for word in word_tokenize(sentence,language='spanish') if word.isalnum() and not word.isdigit()]\n",
    "    return sentences\n",
    "\n",
    "def common_words(entity1,entity2):\n",
    "    words1 = entity1.split()\n",
    "    words2 = entity2.split()\n",
    "    for word in words1:\n",
    "        if word in words2: return True\n",
    "    return False\n",
    "\n",
    "def process_ppl_org(entities,len_text):\n",
    "    counts = dict()\n",
    "    cleaned_entities = list()\n",
    "    for entity in entities: \n",
    "        add = True\n",
    "        if entity.label_=='ORG':\n",
    "            cleaned_entities.append(entity.text)\n",
    "            continue\n",
    "        if entity.label_=='PER': \n",
    "            for r in counts: \n",
    "                if common_words(r,entity.text): \n",
    "                    counts[r]+=1\n",
    "                    add=False\n",
    "                    if len(entity.text.split())>len(r.split()): \n",
    "                        counts[entity.text] = counts[r]\n",
    "                        del counts[r]\n",
    "                    break\n",
    "            if add: counts[entity.text] = 1\n",
    "    if len(counts)>0:\n",
    "        average_appearance = np.mean(list(counts.values()))\n",
    "        all_words = list()\n",
    "        for entity,count in counts.items():\n",
    "            if count>=average_appearance: \n",
    "                cleaned_entities.append(entity)\n",
    "    for i,entity in enumerate(cleaned_entities): \n",
    "        cleaned_entities[i] = ' '.join([word for word in entity.split() if word.isalnum()])\n",
    "    return cleaned_entities\n",
    "\n",
    "def process_places(places,len_text,stopw):\n",
    "    keeps = list()\n",
    "    average_count = np.mean(list(places.values()))\n",
    "    for place,counts in places.items(): \n",
    "        if counts>=average_count: \n",
    "            keeps.append('.'.join([word.lower() for word in place.split() if word.lower() not in stopw]))\n",
    "    return keeps\n",
    "\n",
    "def get_spacy_entities(raw_text,stopw,nlp):\n",
    "    sentences = get_sentences(raw_text,punctuation=True)\n",
    "    len_text = sum([len(sentence) for sentence in sentences])\n",
    "    entities = list()\n",
    "    places = dict()\n",
    "    for sentence in sentences: \n",
    "        sentence = ' '.join(sentence)\n",
    "        doc = nlp(sentence)\n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ in ['PER','ORG'] and len(entity.text.split())<=2: entities.append(entity)\n",
    "            if entity.label_ in ['GPE','LOC']: \n",
    "                try: places[entity.text] += 1\n",
    "                except KeyError: places[entity.text] = 1\n",
    "    entities = process_ppl_org(entities,len_text)\n",
    "    for i,entity in enumerate(entities):\n",
    "        entities[i] = '.'.join([word.lower() for word in entity.split() if word.lower() not in stopw])\n",
    "    places = process_places(places,len_text,stopw)\n",
    "    return entities+places\n",
    "\n",
    "def process_unknowns(unknowns):\n",
    "    keeps = dict()\n",
    "    for word,tag in unknowns:\n",
    "        try: keeps[word] += 1\n",
    "        except KeyError: keeps[word] = 1\n",
    "    average = np.mean(list(keeps.values()))\n",
    "    return [word for word,count in keeps.items() if count>=average and word.isalpha()]\n",
    "\n",
    "def get_spacy_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp):\n",
    "    entities = get_spacy_entities(raw_text,stopw,nlp)\n",
    "    doubles = list()\n",
    "    for entity in entities: doubles += entity.split('.')\n",
    "    doubles = [tag for tag in doubles]\n",
    "    significants,tfidfs,unknowns = get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp)\n",
    "    tags = list()\n",
    "    for i,s in enumerate(significants): \n",
    "        if tfidfs[i]<.0005: break\n",
    "        if s not in doubles: tags.append(s)\n",
    "    tags += [word for word in process_unknowns(unknowns) if word not in doubles]\n",
    "    tags += entities\n",
    "    tags = remove_unwanted(tags)\n",
    "    return list(set(tags))\n",
    "\n",
    "def get_tags(raw_text):\n",
    "    raw_text = raw_text.replace('¡','')\n",
    "    vocab,len_vocab,word_to_int,idfs_array,stopw,nlp = initialize_variables()\n",
    "    return get_spacy_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text): 788 \n",
      "\n",
      "#interamericano\n",
      "#diplomático\n",
      "#china\n",
      "#kimberly.breier\n",
      "#asiático\n",
      "#subsecretario\n",
      "#ricardo.hausmann\n",
      "#aliar\n",
      "#vicepresidente\n",
      "#accionista\n",
      "\n",
      "time: 0.6196033954620361\n"
     ]
    }
   ],
   "source": [
    "def open_link(link):\n",
    "    article = Article(link)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text,article.title\n",
    "\n",
    "link = \"http://www.el-nacional.com/noticias/mundo/los-accionistas-del-bid-rechaza-oposicion-china-hausmann_276065\"\n",
    "text,title = open_link(link)\n",
    "\n",
    "print('len(text):',len(text),'\\n')\n",
    "\n",
    "start = time.time()\n",
    "tags = get_tags(title+' '+text)\n",
    "print('#'+'\\n#'.join(tags))\n",
    "print('\\ntime:',time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
