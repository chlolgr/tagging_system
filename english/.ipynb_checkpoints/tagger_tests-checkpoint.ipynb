{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk import word_tokenize,pos_tag\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import pickle as pkl\n",
    "import spacy\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_variables():\n",
    "    idfs = pkl.load(open('idfs_dict_lemm.pkl','rb'))\n",
    "    vocab = list(idfs.keys())\n",
    "    len_vocab = len(vocab)\n",
    "    word_to_int = dict(zip(vocab,range(len_vocab)))\n",
    "    idfs_array = np.zeros(len_vocab)\n",
    "    for word in vocab: idfs_array[word_to_int[word]] = idfs[word]\n",
    "    stopw = stopwords.words('english')\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    return vocab,len_vocab,word_to_int,idfs_array,stopw,lemmer,nlp\n",
    "\n",
    "def pt_to_wn(tag):\n",
    "    if tag.startswith('J'): return wordnet.ADJ\n",
    "    if tag.startswith('V'): return wordnet.VERB\n",
    "    if tag.startswith('N'): return wordnet.NOUN\n",
    "    if tag.startswith('R'): return wordnet.ADV\n",
    "\n",
    "def remove_unwanted(tags):\n",
    "    remove = list()\n",
    "    days = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
    "    day_of_the_month_re = '[1-9][1-9]?th'\n",
    "    time_re = '[0-1]?[0-9]:[0-5][0-9]([a,p]m)?'\n",
    "    belonging_re = '\\.\\'s'\n",
    "    for i,tag in enumerate(tags): \n",
    "        match = re.search(belonging_re,tag)\n",
    "        if match:\n",
    "            start,end = match.span()\n",
    "            tags[i] = tag[:start]+tag[end:]\n",
    "        for bit in tag.split('.'):\n",
    "            match = re.search(day_of_the_month_re,bit)\n",
    "            if match: \n",
    "                remove.append(tag)\n",
    "                break\n",
    "            match = re.search(time_re,bit)\n",
    "            if match: remove.append(tag)\n",
    "    for tag in remove: tags.remove(tag)\n",
    "    return [tag for tag in tags if tag not in days and len(tag)>1]\n",
    "\n",
    "def get_clean_text_raw(raw_text,stopw,lemmer):\n",
    "    text = word_tokenize(raw_text)\n",
    "    postags = pos_tag(text)\n",
    "    postags = [(word,tag) for word,tag in postags if tag in ['NN','JJ','JJR','NNS','VB','VBD','VBG','VBN','VBP','VBZ']]\n",
    "    text = [lemmer.lemmatize(word.lower(),pt_to_wn(tag)) for word,tag in postags if word.lower() not in stopw and word.isalnum() and not word.isdigit()]\n",
    "    postags = [tag for word,tag in postags if word.lower() not in stopw and word.isalnum() and not word.isdigit()]\n",
    "    return text,postags\n",
    "\n",
    "def get_TFs_raw(raw_text,len_vocab,word_to_int,stopw,lemmer):\n",
    "    words,postags = get_clean_text_raw(raw_text,stopw,lemmer)\n",
    "    word_counts = dict(zip(*np.unique(words,return_counts=True)))\n",
    "    n_words = sum(list(word_counts.values()))\n",
    "    tfs = np.zeros(len_vocab)\n",
    "    unknowns = list()\n",
    "    for i,word in enumerate(words): \n",
    "        try: tfs[word_to_int[word]] = word_counts[word]/n_words\n",
    "        except KeyError: unknowns.append((word,postags[i]))\n",
    "    return tfs,unknowns\n",
    "\n",
    "def get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,lemmer):\n",
    "    tfs,unknowns = get_TFs_raw(raw_text,len_vocab,word_to_int,stopw,lemmer)\n",
    "    tfidfs = tfs*idfs_array\n",
    "    order = np.flip(np.argsort(tfidfs))\n",
    "    significants = np.array(vocab)[order]\n",
    "    return list(significants),list(tfidfs[order]),unknowns\n",
    "\n",
    "def get_significants_from_title(raw_title,word_to_int,lemmer):\n",
    "    ptags = pos_tag(word_tokenize(raw_title))\n",
    "    keeps = list()\n",
    "    for word,tag in ptags:\n",
    "        if tag.startswith('N') and word.isalnum() and not word.isdigit():\n",
    "            word = lemmer.lemmatize(word.lower())\n",
    "            try: \n",
    "                word_to_int[word]\n",
    "                keeps.append(word)\n",
    "            except KeyError: pass\n",
    "    return keeps\n",
    "\n",
    "def get_sentences(raw_text,punctuation=False):\n",
    "    raw_text = raw_text.replace('-','')\n",
    "    sentences = list()\n",
    "    sentence = ''\n",
    "    for character in raw_text:\n",
    "        if character in ['.','!','?']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = ''\n",
    "        else: sentence+=character\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        if punctuation: sentences[i] = [word for word in word_tokenize(sentence)]\n",
    "        else: sentences[i] = [word for word in word_tokenize(sentence) if word.isalnum() and not word.isdigit()]\n",
    "    return sentences\n",
    "\n",
    "def common_words(entity1,entity2):\n",
    "    words1 = entity1.split()\n",
    "    words2 = entity2.split()\n",
    "    for word in words1:\n",
    "        if word in words2: return True\n",
    "    return False\n",
    "\n",
    "def process_ppl_org(entities,len_text):\n",
    "    counts = dict()\n",
    "    cleaned_entities = list()\n",
    "    for entity in entities: \n",
    "        add = True\n",
    "        if entity.label_=='ORG':\n",
    "            cleaned_entities.append(entity.text)\n",
    "            continue\n",
    "        if entity.label_=='PERSON': \n",
    "            for r in counts: \n",
    "                if common_words(r,entity.text): \n",
    "                    counts[r]+=1\n",
    "                    add=False\n",
    "                    if len(entity.text.split())>len(r.split()): \n",
    "                        counts[entity.text] = counts[r]\n",
    "                        del counts[r]\n",
    "                    break\n",
    "            if add: counts[entity.text] = 1\n",
    "    if len(counts)>0:\n",
    "        average_appearance = np.mean(list(counts.values()))\n",
    "        all_words = list()\n",
    "        for entity,count in counts.items():\n",
    "            if count>=average_appearance: \n",
    "                cleaned_entities.append(entity)\n",
    "    for i,entity in enumerate(cleaned_entities): \n",
    "        cleaned_entities[i] = ' '.join([word for word in entity.split() if word.isalnum()])\n",
    "    return cleaned_entities\n",
    "\n",
    "def process_places(places,len_text,stopw):\n",
    "    keeps = list()\n",
    "    average_count = np.mean(list(places.values()))\n",
    "    for place,counts in places.items(): \n",
    "        if counts>=average_count: \n",
    "            keeps.append('.'.join([word.lower() for word in place.split() if word.lower() not in stopw]))\n",
    "    return keeps\n",
    "\n",
    "def get_spacy_entities(raw_text,stopw,nlp,lemmer):\n",
    "    sentences = get_sentences(raw_text,punctuation=True)\n",
    "    len_text = sum([len(sentence) for sentence in sentences])\n",
    "    entities = list()\n",
    "    places = dict()\n",
    "    for sentence in sentences: \n",
    "        sentence = ' '.join(sentence)\n",
    "        doc = nlp(sentence)\n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ in ['PERSON','ORG'] and len(entity.text.split())<=2: entities.append(entity)\n",
    "            if entity.label_=='GPE': \n",
    "                try: places[entity.text] += 1\n",
    "                except KeyError: places[entity.text] = 1\n",
    "    entities = process_ppl_org(entities,len_text)\n",
    "    for i,entity in enumerate(entities):\n",
    "        entities[i] = '.'.join([word.lower() for word in entity.split() if word.lower() not in stopw])\n",
    "    places = process_places(places,len_text,stopw)\n",
    "    return entities+places\n",
    "\n",
    "def process_unknowns(unknowns):\n",
    "    porter = PorterStemmer()\n",
    "    keeps = list()\n",
    "    stems = dict()\n",
    "    for word,tag in unknowns:\n",
    "        stem = porter.stem(word)\n",
    "        try: stems[stem].append((word,tag))\n",
    "        except KeyError: stems[stem] = [(word,tag)]\n",
    "    for stem,appearances in stems.items():\n",
    "        if len(appearances)>1:\n",
    "            done = False\n",
    "            for word,tag in appearances:\n",
    "                if tag=='NN': \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances:\n",
    "                if tag.startswith('N'): \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances:\n",
    "                if tag=='JJ': \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances:\n",
    "                if tag.startswith('J'): \n",
    "                    keeps.append(word)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done: continue\n",
    "            for word,tag in appearances: \n",
    "                if tag.startswith('V'): \n",
    "                    keeps.append(word)\n",
    "                    break\n",
    "    return keeps\n",
    "\n",
    "def get_spacy_significants(raw_text,lemmer,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp):\n",
    "    entities = get_spacy_entities(raw_text,stopw,nlp,lemmer)\n",
    "    doubles = list()\n",
    "    for entity in entities: doubles += entity.split('.')\n",
    "    doubles = [tag for tag in doubles]\n",
    "    significants,tfidfs,unknowns = get_significants(raw_text,vocab,idfs_array,len_vocab,word_to_int,stopw,lemmer)\n",
    "    tags = list()\n",
    "    for i,s in enumerate(significants): \n",
    "        if tfidfs[i]<.0005: break\n",
    "        if s not in doubles: tags.append(s)\n",
    "    tags += process_unknowns(unknowns)\n",
    "    tags += entities\n",
    "    tags = remove_unwanted(tags)\n",
    "    return list(set(tags))\n",
    "\n",
    "def get_tags(raw_text,raw_title):\n",
    "    vocab,len_vocab,word_to_int,idfs_array,stopw,lemmer,nlp = initialize_variables()\n",
    "    title_tags = get_significants_from_title(raw_title,word_to_int,lemmer)\n",
    "    text_tags = get_spacy_significants(raw_text,lemmer,vocab,idfs_array,len_vocab,word_to_int,stopw,nlp)\n",
    "    return list(set(title_tags+text_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#notredame.cathedral\n",
      "#french\n",
      "#prince.philip\n",
      "#flame\n",
      "#spire\n",
      "#sustained\n",
      "#countryman\n",
      "#scaffold\n",
      "#bronze\n",
      "#sincere\n",
      "#cathedral\n",
      "#president\n",
      "#landmark\n",
      "#renovation\n",
      "#statement\n",
      "#rip\n",
      "#catastrophe\n",
      "#france\n",
      "#admiration\n",
      "#blaze\n",
      "#fire\n",
      "#engulf\n",
      "#macron\n",
      "#paris\n",
      "#queen\n",
      "\n",
      "time: 2.526322841644287\n"
     ]
    }
   ],
   "source": [
    "def open_link(link):\n",
    "    article = Article(link)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text,article.title\n",
    "\n",
    "\n",
    "link = \"https://www.foxnews.com/entertainment/queen-elizabeth-notre-dame-fire-saddened\"\n",
    "text,title = open_link(link)\n",
    "\n",
    "start = time.time()\n",
    "print('#'+'\\n#'.join(get_tags(text,title)))\n",
    "print('\\ntime:',time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
